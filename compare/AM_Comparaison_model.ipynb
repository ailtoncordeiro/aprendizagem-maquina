{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   popularity  duration_ms  danceability  energy       key  loudness  mode  \\\n",
      "0        0.73     0.042473      0.686294  0.4610  0.090909  0.791392   0.0   \n",
      "1        0.55     0.026971      0.426396  0.1660  0.090909  0.597377   1.0   \n",
      "2        0.57     0.038679      0.444670  0.3590  0.000000  0.736123   1.0   \n",
      "3        0.71     0.036978      0.270051  0.0596  0.000000  0.573701   1.0   \n",
      "4        0.82     0.036389      0.627411  0.4430  0.181818  0.737103   1.0   \n",
      "\n",
      "   speechiness  acousticness  instrumentalness  liveness   valence     tempo  \\\n",
      "0     0.148187      0.032329          0.000001    0.3580  0.718593  0.361245   \n",
      "1     0.079067      0.927711          0.000006    0.1010  0.268342  0.318397   \n",
      "2     0.057720      0.210843          0.000000    0.1170  0.120603  0.313643   \n",
      "3     0.037617      0.908635          0.000071    0.1320  0.143719  0.746758   \n",
      "4     0.054508      0.470884          0.000000    0.0829  0.167839  0.492863   \n",
      "\n",
      "   time_signature  \n",
      "0             0.8  \n",
      "1             0.8  \n",
      "2             0.8  \n",
      "3             0.6  \n",
      "4             0.8  \n",
      "Tamanho do vetor: 113999\n",
      "Tamanho do vetor 50% 56999\n",
      "56999 14\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "# USANDO O SVM NA BASE DE DADOS DO DIABETES\n",
    "# ESCOLHENDO VARIOS Kernels PARA A BASE\n",
    "#\n",
    "\n",
    "import pandas\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.colors as mcolors\n",
    "import os\n",
    "# Importer ces fonctions pour des calculs de métriques spécifiques\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from math import ceil\n",
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "\n",
    "#definindo os nomes de cada coluna\n",
    "names = ['num-pregnant', 'glucose', 'diastolic', 'triceps-skin', 'insulin', 'body-mass', 'diabetes-pedigree', 'age', 'class']\n",
    "\n",
    "dataset = pandas.read_csv(\"/Users/nathanjacas/Documents/Cours/AM/Avaliaçao Comparativa/dataset.csv\")\n",
    "\n",
    "#########################################################################\n",
    "######                     Preparação de Dados                  #########\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "\n",
    "#Limpeza do data frame de dados faltantes\n",
    "dataset = dataset.dropna(axis=0, how='any')\n",
    "\n",
    "#print(\"Primeiros dados\")\n",
    "#print(dataset.head(5))\n",
    "\n",
    "#Criação de um dataframe menor sem a ultima coluna\n",
    "datasetfeat = dataset.iloc[:, 1:-1]\n",
    "\n",
    "#Eliminação das colunas com dados não numéricos\n",
    "dataset1 = datasetfeat.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "df_max = dataset1.max(axis=0)\n",
    "df_min = dataset1.min(axis=0)\n",
    "#print('Max:',df_max)\n",
    "#print('Min:',df_min)\n",
    "\n",
    "#Ajuste de escala\n",
    "for coluna in dataset1.columns:\n",
    "    dataset1[coluna] = (dataset1[coluna] - df_min[coluna]) / (df_max[coluna] - df_min[coluna])\n",
    "\n",
    "# Exibindo o DataFrame\n",
    "print(dataset1.head(5))\n",
    "\n",
    "\n",
    "#divisao de dados atributos e classe\n",
    "X = dataset1.values[:,:]  # 0:-1] #caracteristicas (features)\n",
    "Y = dataset.values[:, -1] #classe a ser analisada: track_genre\n",
    "\n",
    "#Utilizar o 50% dos dados\n",
    "print('Tamanho do vetor:',len(Y))\n",
    "d50 = int(0.50*len(Y))\n",
    "\n",
    "X = X[0:d50,:]\n",
    "Y = Y[0:d50]\n",
    "\n",
    "print('Tamanho do vetor 50%',len(Y))\n",
    "\n",
    "\n",
    "print(X.shape[0],X.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#construindo conjuntos de treinamento, validacao e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.25, random_state = 10)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.25, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_scorer.py:548: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#########################################################################\n",
    "######                     Decision Tree                        #########\n",
    "#########################################################################\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Configuration de la validation croisée stratifiée\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "# Définir le modèle d'arbre de décision avec critère d'entropie\n",
    "clf = DecisionTreeClassifier(criterion='entropy', random_state=10)\n",
    "\n",
    "# Définir les métriques à évaluer\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted'),\n",
    "    'f1': make_scorer(f1_score, average='weighted'),\n",
    "    'roc_auc': make_scorer(roc_auc_score, needs_proba=True, multi_class=\"ovr\")\n",
    "}\n",
    "\n",
    "# Exécution de la validation croisée\n",
    "results = cross_validate(clf, X_train, y_train, cv=cv, scoring=scoring, return_train_score=False, n_jobs=-1)\n",
    "\n",
    "# Formatage et sauvegarde des résultats\n",
    "results_df = pd.DataFrame()\n",
    "file_path = '/Users/nathanjacas/Documents/Cours/AM/Avaliaçao Comparativa/Comparaison_utility/model_performance.csv'\n",
    "for metric, scores in results.items():\n",
    "    if 'test_' in metric:\n",
    "        for fold_index, score in enumerate(scores):\n",
    "            new_row = pd.DataFrame({\n",
    "                'Model': ['Decision Tree'],  # Changement du modèle pour 'Decision Tree'\n",
    "                'Metric': [metric.split('test_')[1]],\n",
    "                'Fold': [fold_index + 1],\n",
    "                'Score': [score]\n",
    "            })\n",
    "            results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "# Vérification de l'existence du fichier et de son contenu\n",
    "header = not os.path.exists(file_path) or os.stat(file_path).st_size == 0\n",
    "\n",
    "# Écriture dans le fichier CSV\n",
    "results_df.to_csv(file_path, mode='a', header=header, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_scorer.py:548: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 27\u001b[0m\n\u001b[1;32m     18\u001b[0m scoring \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m: make_scorer(precision_score, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m: make_scorer(roc_auc_score, needs_proba\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, multi_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m }\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Exécution de la validation croisée\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Formatage et sauvegarde des résultats\u001b[39;00m\n\u001b[1;32m     30\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:430\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 430\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "######                     SVM                                  #########\n",
    "#########################################################################\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Configuration de la validation croisée stratifiée\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "# Définir le modèle SVM avec le kernel poly\n",
    "clf = SVC(C=30, kernel='poly', probability=True, random_state=10)\n",
    "\n",
    "# Définir les métriques à évaluer\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted'),\n",
    "    'f1': make_scorer(f1_score, average='weighted'),\n",
    "    'roc_auc': make_scorer(roc_auc_score, needs_proba=True, multi_class=\"ovr\")\n",
    "}\n",
    "\n",
    "# Exécution de la validation croisée\n",
    "results = cross_validate(clf, X_train, y_train, cv=cv, scoring=scoring, return_train_score=False, n_jobs=-1)\n",
    "\n",
    "# Formatage et sauvegarde des résultats\n",
    "results_df = pd.DataFrame()\n",
    "file_path = '/Users/nathanjacas/Documents/Cours/AM/Avaliaçao Comparativa/Comparaison_utility/model_performance.csv'\n",
    "for metric, scores in results.items():\n",
    "    if 'test_' in metric:\n",
    "        for fold_index, score in enumerate(scores):\n",
    "            new_row = pd.DataFrame({\n",
    "                'Model': ['SVM'],\n",
    "                'Metric': [metric.split('test_')[1]],\n",
    "                'Fold': [fold_index + 1],\n",
    "                'Score': [score]\n",
    "            })\n",
    "            results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "# Check if the file exists and has content\n",
    "header = not os.path.exists(file_path) or os.stat(file_path).st_size == 0\n",
    "\n",
    "# Writing to CSV\n",
    "results_df.to_csv(file_path, mode='a', header=header, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_scorer.py:548: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "######                        KNN                               #########\n",
    "#########################################################################\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Configuration de la validation croisée stratifiée\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "# Définir le modèle KNN\n",
    "clf = KNeighborsClassifier(n_neighbors=5)  # Vous pouvez ajuster le nombre de voisins si nécessaire\n",
    "\n",
    "# Définir les métriques à évaluer\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted'),\n",
    "    'f1': make_scorer(f1_score, average='weighted'),\n",
    "    'roc_auc': make_scorer(roc_auc_score, needs_proba=True, multi_class=\"ovr\")\n",
    "}\n",
    "\n",
    "# Exécution de la validation croisée\n",
    "results = cross_validate(clf, X_train, y_train, cv=cv, scoring=scoring, return_train_score=False, n_jobs=-1)\n",
    "\n",
    "# Formatage et sauvegarde des résultats\n",
    "results_df = pd.DataFrame()\n",
    "file_path = '/Users/nathanjacas/Documents/Cours/AM/Avaliaçao Comparativa/Comparaison_utility/model_performance.csv'\n",
    "for metric, scores in results.items():\n",
    "    if 'test_' in metric:\n",
    "        for fold_index, score in enumerate(scores):\n",
    "            new_row = pd.DataFrame({\n",
    "                'Model': ['KNN'],\n",
    "                'Metric': [metric.split('test_')[1]],\n",
    "                'Fold': [fold_index + 1],\n",
    "                'Score': [score]\n",
    "            })\n",
    "            results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "# Vérification de l'existence du fichier et de son contenu\n",
    "header = not os.path.exists(file_path) or os.stat(file_path).st_size == 0\n",
    "\n",
    "# Écriture dans le fichier CSV\n",
    "results_df.to_csv(file_path, mode='a', header=header, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_scorer.py:548: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "######                        MLP                               #########\n",
    "#########################################################################\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Configuration de la validation croisée stratifiée\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "# Définir le modèle MLP\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', random_state=10)\n",
    "\n",
    "# Définir les métriques à évaluer\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted'),\n",
    "    'f1': make_scorer(f1_score, average='weighted'),\n",
    "    # ROC AUC peut nécessiter des modifications pour les classes multiples ou des approches spécifiques\n",
    "    'roc_auc': make_scorer(roc_auc_score, needs_proba=True, multi_class=\"ovr\")\n",
    "}\n",
    "\n",
    "# Exécution de la validation croisée\n",
    "results = cross_validate(clf, X_train, y_train, cv=cv, scoring=scoring, return_train_score=False, n_jobs=-1)\n",
    "\n",
    "# Formatage et sauvegarde des résultats\n",
    "results_df = pd.DataFrame()\n",
    "file_path = '/Users/nathanjacas/Documents/Cours/AM/Avaliaçao Comparativa/Comparaison_utility/model_performance.csv'  # Modifiez le chemin selon votre environnement\n",
    "for metric, scores in results.items():\n",
    "    if 'test_' in metric:\n",
    "        for fold_index, score in enumerate(scores):\n",
    "            new_row = pd.DataFrame({\n",
    "                'Model': ['MLP'],\n",
    "                'Metric': [metric.split('test_')[1]],\n",
    "                'Fold': [fold_index + 1],\n",
    "                'Score': [score]\n",
    "            })\n",
    "            results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "# Check if the file exists and has content\n",
    "header = not os.path.exists(file_path) or os.stat(file_path).st_size == 0\n",
    "\n",
    "# Writing to CSV\n",
    "results_df.to_csv(file_path, mode='a', header=header, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vetor: 113999\n",
      "Tamanho do vetor 100% 113999\n",
      "[0.008552631578947369, 0.008771929824561403, 0.008771929824561403, 0.008698830409356725, 0.008845675853498063]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from random import seed, randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import make_scorer,roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual))\n",
    "\n",
    "# Calculate precision\n",
    "def precision_metric(actual, predicted):\n",
    "    true_positives = sum((act == 1 and pred == 1) for act, pred in zip(actual, predicted))\n",
    "    false_positives = sum((act == 0 and pred == 1) for act, pred in zip(actual, predicted))\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0\n",
    "    return precision\n",
    "\n",
    "# Calculate recall\n",
    "def recall_metric(actual, predicted):\n",
    "    true_positives = sum((act == 1 and pred == 1) for act, pred in zip(actual, predicted))\n",
    "    false_negatives = sum((act == 1 and pred == 0) for act, pred in zip(actual, predicted))\n",
    "\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0\n",
    "    return recall\n",
    "\n",
    "# Calculate F1 Score\n",
    "def f1_score_metric(actual, predicted):\n",
    "    true_positives = sum((act == 1 and pred == 1) for act, pred in zip(actual, predicted))\n",
    "    false_positives = sum((act == 0 and pred == 1) for act, pred in zip(actual, predicted))\n",
    "    false_negatives = sum((act == 1 and pred == 0) for act, pred in zip(actual, predicted))\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) != 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) != 0 else 0\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    return f1\n",
    "\n",
    "# Calculate ROC AUC\n",
    "def roc_auc_metric(actual, predicted):\n",
    "    total_positive = sum(1 for val in actual if val == 1)\n",
    "    total_negative = sum(1 for val in actual if val == 0)\n",
    "\n",
    "    # Criar uma lista de tuplas (predicted_probability, true_class)\n",
    "    predictions = list(zip(predicted, actual))\n",
    "\n",
    "    # Classificar as previsões com base na probabilidade prevista (em ordem decrescente)\n",
    "    predictions_sorted = sorted(predictions, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Inicializar os contadores para TPR e FPR\n",
    "    true_positive_count = 0\n",
    "    false_positive_count = 0\n",
    "\n",
    "    # Inicializar os valores anteriores para o cálculo do AUC\n",
    "    prev_true_positive_count = 0\n",
    "    prev_false_positive_count = 0\n",
    "\n",
    "    # Inicializar a área sob a curva (AUC)\n",
    "    auc = 0.0\n",
    "\n",
    "    # Iterar sobre as previsões classificadas\n",
    "    for pred_prob, true_class in predictions_sorted:\n",
    "        # Atualizar os contadores TPR e FPR\n",
    "        if true_class == 1:\n",
    "            true_positive_count += 1\n",
    "        else:\n",
    "            false_positive_count += 1\n",
    "\n",
    "        # Calcular as taxas TPR e FPR\n",
    "        tpr = true_positive_count / total_positive\n",
    "        fpr = false_positive_count / total_negative\n",
    "\n",
    "        # Calcular a contribuição para o AUC usando a regra trapezoidal\n",
    "        auc += ((fpr - prev_false_positive_count) * (tpr + prev_true_positive_count)) / 2\n",
    "\n",
    "        # Atualizar os valores anteriores\n",
    "        prev_true_positive_count = tpr\n",
    "        prev_false_positive_count = fpr\n",
    "\n",
    "    if auc>1 :\n",
    "        auc = 1\n",
    "    return auc\n",
    "\n",
    "\n",
    "def evaluate_algorithm(dataset, algorithm, n_splits, *args):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    rocauc_scores = []\n",
    "    \n",
    "    dataset = np.array(dataset)  # Convertir dataset en tableau numpy\n",
    "    \n",
    "    for train_index, test_index in skf.split(dataset[:, :-1], dataset[:, -1]):  # Utiliser les caractéristiques et les étiquettes séparément\n",
    "        train_set = dataset[train_index]\n",
    "        test_set = dataset[test_index]\n",
    "        \n",
    "        train_set_X = train_set[:, :-1]\n",
    "        train_set_y = train_set[:, -1]\n",
    "        test_set_X = test_set[:, :-1]\n",
    "        test_set_y = test_set[:, -1]\n",
    "        \n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        \n",
    "        accuracy = accuracy_metric(test_set_y, predicted)\n",
    "        precision = precision_metric(test_set_y, predicted)\n",
    "        recall = recall_metric(test_set_y, predicted)\n",
    "        f1 = f1_score_metric(test_set_y, predicted)\n",
    "        rocaux = roc_auc_metric(test_set_y, predicted)\n",
    "        \n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        rocauc_scores.append(rocaux)\n",
    "        \n",
    "    return accuracy_scores, precision_scores, recall_scores, f1_scores, rocauc_scores\n",
    "\n",
    "# LVQ Algorithm\n",
    "def learning_vector_quantization(train, test, n_codebooks, lrate, epochs):\n",
    "    codebooks = train_codebooks(train, n_codebooks, lrate, epochs)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        output = predict(codebooks, row)\n",
    "        predictions.append(output)\n",
    "    return predictions\n",
    "\n",
    "# calculate the Euclidean distance between two vectors\n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tav= 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t  distance = distance + (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    "\n",
    "\n",
    "# Locate the best matching unit\n",
    "def get_best_matching_unit(codebooks, test_row):\n",
    "    distances = [(codebook, euclidean_distance(codebook, test_row)) for codebook in codebooks]\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    return distances[0][0]\n",
    "\n",
    "# Make a prediction with codebook vectors\n",
    "def predict(codebooks, test_row):\n",
    "    bmu = get_best_matching_unit(codebooks, test_row)\n",
    "    return bmu[-1]\n",
    "\n",
    "# Create a random codebook vector\n",
    "def random_codebook(train):\n",
    "    n_records = len(train)\n",
    "    n_features = len(train[0])\n",
    "    return [train[randrange(n_records)][i] for i in range(n_features)]\n",
    "\n",
    "# Train a set of codebook vectors\n",
    "def train_codebooks(train, n_codebooks, lrate, epochs):\n",
    "    codebooks = [random_codebook(train) for _ in range(n_codebooks)]\n",
    "    for epoch in range(epochs):\n",
    "        rate = lrate * (1.0-(epoch/float(epochs)))\n",
    "        for row in train:\n",
    "            bmu = get_best_matching_unit(codebooks, row)\n",
    "            for i in range(len(row)-1):\n",
    "                error = row[i] - bmu[i]\n",
    "                if bmu[-1] == row[-1]:\n",
    "                    bmu[i] += rate * error\n",
    "                else:\n",
    "                    bmu[i] -= rate * error\n",
    "    return codebooks\n",
    "\n",
    "\n",
    "#######################################################\n",
    "######          Preparação de Dados           #########\n",
    "#######################################################\n",
    "\n",
    "seed(1)\n",
    "\n",
    "# Carregar a base de dados\n",
    "filename = \"/Users/nathanjacas/Documents/Cours/AM/Avaliaçao Comparativa/dataset.csv\"\n",
    "dataset0 = pandas.read_csv(filename)\n",
    "\n",
    "\n",
    "#Limpeza do data frame de dados faltantes\n",
    "dataset0 = dataset0.dropna(axis=0, how='any')\n",
    "\n",
    "#Criação de um dataframe menor sem a primeira e ultima coluna\n",
    "datasetfeat = dataset0.iloc[:, 1:-1]\n",
    "\n",
    "#Eliminação das colunas com dados não numéricos\n",
    "dataset2 = datasetfeat.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "df_max = dataset2.max(axis=0)\n",
    "df_min = dataset2.min(axis=0)\n",
    "\n",
    "#Adimensionalização das features\n",
    "for coluna in dataset2.columns:\n",
    "    dataset2[coluna] = dataset2[coluna] / (df_max[coluna] - df_min[coluna])\n",
    "\n",
    "#divisao de dados atributos e classe\n",
    "X = dataset2.values[:,:]  # 0:-1] #caracteristicas (features)\n",
    "Y = dataset0.values[:, -1] #classe a ser analisada: track_genre\n",
    "\n",
    "#Utilizar o 100% dos dados\n",
    "print('Tamanho do vetor:',len(Y))\n",
    "d50 = int(len(Y))\n",
    "\n",
    "X = X[0:d50,:]\n",
    "Y = Y[0:d50]\n",
    "\n",
    "print('Tamanho do vetor 100%',len(Y))\n",
    "\n",
    "#Criação de conjuntos de treinamento, validacao e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.40, random_state = 10)\n",
    "\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size = 0.50, random_state = 10)\n",
    "\n",
    "\n",
    "#Conjunto de Treinamento\n",
    "#Juntar a matriz de features com o vetor de classes, o vetor de classes é posicionado no final\n",
    "dataset3 = np.hstack((X_train, y_train[:, np.newaxis]))\n",
    "\n",
    "#Transforma o array Python para uma lista\n",
    "dataset4 = dataset3.tolist()\n",
    "\n",
    "#Transforma os valores da lista em arrayas\n",
    "dataset =[]\n",
    "for valores in dataset4:\n",
    "  arrays = list(map(str, valores))\n",
    "  dataset.append(arrays)\n",
    "\n",
    "#Transforma os elementos numéricos da lista em números reais\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "#######################################################\n",
    "######       Fim da Preparação de Dados       #########\n",
    "#######################################################\n",
    "\n",
    "\n",
    "#######################################################\n",
    "######          Avaliação do algoritmo LVQ    #########\n",
    "#######################################################\n",
    "\n",
    "\n",
    "scores_nfolds = {'nfolds':[],'accuracy': [], 'precision': [], 'recall': [], 'f1_score': [], 'Roc_Auc':[]}\n",
    "\n",
    "n_folds = 5\n",
    "learn_rate = 0.1\n",
    "n_epochs = 1\n",
    "n_codebooks = 30 #15\n",
    "\n",
    "accuracy_scores, precision_scores, recall_scores, f1_scores,rocauc_scores = evaluate_algorithm(dataset, learning_vector_quantization, n_folds, n_codebooks, learn_rate, n_epochs)\n",
    "\n",
    "print(accuracy_scores)\n",
    "print(precision_scores)\n",
    "print(recall_scores)\n",
    "\n",
    "# Formatage et sauvegarde des résultats\n",
    "results_df = pd.DataFrame()\n",
    "file_path = '/Users/nathanjacas/Documents/Cours/AM/Avaliaçao Comparativa/Comparaison_utility/model_performance.csv'  # Modifiez le chemin selon votre environnement\n",
    "\n",
    "# Récupérer les scores\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "scores_lists = [accuracy_scores, precision_scores, recall_scores, f1_scores, rocauc_scores]\n",
    "\n",
    "\n",
    "# Créer un DataFrame pour chaque métrique\n",
    "for metric, scores in zip(metrics, scores_lists):\n",
    "    for fold_index, score in enumerate(scores):\n",
    "        new_row = pd.DataFrame({\n",
    "            'Model': ['LVQ'],\n",
    "            'Metric': [metric],\n",
    "            'Fold': [fold_index + 1],\n",
    "            'Score': [score]\n",
    "        })\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "# Vérifier si le fichier existe et s'il contient du contenu\n",
    "header = not os.path.exists(file_path) or os.stat(file_path).st_size == 0\n",
    "\n",
    "# Écriture dans un fichier CSV\n",
    "results_df.to_csv(file_path, mode='a', header=header, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Metric            Comparison      Test  Statistic   P-Value\n",
      "0    accuracy  SVM vs Decision Tree  Wilcoxon        0.0  0.001953\n",
      "1    accuracy            SVM vs KNN  Wilcoxon        0.0  0.001953\n",
      "2    accuracy  Decision Tree vs KNN  Wilcoxon        0.0  0.001953\n",
      "3    accuracy            All models  Friedman       20.0  0.000045\n",
      "4   precision  SVM vs Decision Tree  Wilcoxon        0.0  0.001953\n",
      "5   precision            SVM vs KNN  Wilcoxon        0.0  0.001953\n",
      "6   precision  Decision Tree vs KNN  Wilcoxon        0.0  0.001953\n",
      "7   precision            All models  Friedman       20.0  0.000045\n",
      "8      recall  SVM vs Decision Tree  Wilcoxon        0.0  0.001953\n",
      "9      recall            SVM vs KNN  Wilcoxon        0.0  0.001953\n",
      "10     recall  Decision Tree vs KNN  Wilcoxon        0.0  0.001953\n",
      "11     recall            All models  Friedman       20.0  0.000045\n",
      "12         f1  SVM vs Decision Tree  Wilcoxon        0.0  0.001953\n",
      "13         f1            SVM vs KNN  Wilcoxon        0.0  0.001953\n",
      "14         f1  Decision Tree vs KNN  Wilcoxon        0.0  0.001953\n",
      "15         f1            All models  Friedman       20.0  0.000045\n",
      "16    roc_auc  SVM vs Decision Tree  Wilcoxon        0.0  0.001953\n",
      "17    roc_auc            SVM vs KNN  Wilcoxon        0.0  0.001953\n",
      "18    roc_auc  Decision Tree vs KNN  Wilcoxon        0.0  0.001953\n",
      "19    roc_auc            All models  Friedman       20.0  0.000045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qg/_cqjp2wj1_j60qh8lvljg5140000gn/T/ipykernel_44879/1226855431.py:31: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_results = pd.concat([test_results, new_row], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import wilcoxon, friedmanchisquare\n",
    "\n",
    "# Chargement des données\n",
    "data = pd.read_csv('/Users/nathanjacas/Documents/Cours/AM/Avaliaçao Comparativa/Comparaison_utility/model_performance.csv')\n",
    "\n",
    "# Initialiser un DataFrame pour stocker les résultats des tests\n",
    "test_results = pd.DataFrame(columns=['Metric', 'Comparison', 'Test', 'Statistic', 'P-Value'])\n",
    "\n",
    "# Boucle sur chaque métrique\n",
    "for metric in data['Metric'].unique():\n",
    "    metric_data = data[data['Metric'] == metric]\n",
    "    \n",
    "    # Collecter les scores pour chaque modèle et métrique\n",
    "    models_scores = {model: list(metric_data[metric_data['Model'] == model]['Score']) for model in metric_data['Model'].unique()}\n",
    "    \n",
    "    # Comparaisons deux à deux avec le test de Wilcoxon\n",
    "    models = list(models_scores.keys())\n",
    "    for i in range(len(models)):\n",
    "        for j in range(i + 1, len(models)):\n",
    "            scores1 = models_scores[models[i]]\n",
    "            scores2 = models_scores[models[j]]\n",
    "            stat, p = wilcoxon(scores1, scores2)\n",
    "            new_row = pd.DataFrame({\n",
    "                'Metric': [metric],\n",
    "                'Comparison': [f\"{models[i]} vs {models[j]}\"],\n",
    "                'Test': ['Wilcoxon'],\n",
    "                'Statistic': [stat],\n",
    "                'P-Value': [p]\n",
    "            })\n",
    "            test_results = pd.concat([test_results, new_row], ignore_index=True)\n",
    "    \n",
    "    # Test de Friedman pour toutes les métriques si plus de 2 modèles\n",
    "    if len(models) > 2:\n",
    "        friedman_scores = [models_scores[model] for model in models]\n",
    "        stat_f, p_f = friedmanchisquare(*friedman_scores)\n",
    "        new_row = pd.DataFrame({\n",
    "            'Metric': [metric],\n",
    "            'Comparison': ['All models'],\n",
    "            'Test': ['Friedman'],\n",
    "            'Statistic': [stat_f],\n",
    "            'P-Value': [p_f]\n",
    "        })\n",
    "        test_results = pd.concat([test_results, new_row], ignore_index=True)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(test_results)\n",
    "\n",
    "# Optionnel : Sauvegarder les résultats dans un fichier CSV\n",
    "test_results.to_csv('test_results.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
